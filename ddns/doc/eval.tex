\section{Evaluation}

\subsection{Performance}

XXX needs work

\subsubsection{Experimental Setup}

We ran real DNS traces on a simulated DDNS network
of servers to measure load balancing. We set up a
Chord ring of 1000 nodes and seeded the ring with 
real DNS data. Then performed lookup requests.

\subsubsection{Load Balance}

Do a measure of how long it takes to converge to good load 
balancing when there is a burst of lookups for 1 (or more)
popular names. Compare that to the slashdot effects now.

\subsubsection{Latency}

We cannot perform a fair measurement of latency so we will do an
analysis based on counting number of hops between servers
and combining that with real latency to current root DNS servers.

%XXAdding keys to resource records make them bigger
XXPublic key cryptography is an expensive operation. 
DDNS computes pub keys every time.

DDNS is not going to disappear without an answer for a lookup,
unlike current DNS. So we can't really compare this, but should
mention about it.

\subsection{Discussion}

\subsubsection{DNS Administration Problems}

In their 1988 retrospective~\cite{dns}, Mockapetris and Dunlap
listed ``distribution of control vs. distribution of expertise
or responsibility'' as one of the shortcomings of their system,
lamenting:
\begin{quote}
Distributing authority for a database does not distribute
corresponding amounts of expertise.  Maintainers fix things
until they work, rather than until they work well, and want to use,
not understand, the systems they are provided.
Systems designers should anticipate this, and try to
compensate by technical means.
\end{quote}
To justify their point, they cited three failures:
they did not require proof that administrators had set up
truly redundant name servers without a single point of failure;
in the documentation, they used hour-long TTLs 
in the examples but suggested
day-long TTLs in the text, with the result that everyone used
hour-long TTLs; debugging was made difficult by not requiring
servers to identify their server version and type in an automated way.
DDNS provides technical compensation for their first concern,
as well as for other problems.

In their handbook for the Berkeley DNS server, BIND,
Albitz and Liu~\cite{dns-bind} list what they believe to be
thirteen of the most common problems in configuring
a name server.
Of these, our system addresses six of them:
\begin{description}
\item[Forget to signal primary master server.]
There are no primary servers in DDNS, so this cannot happen.
Unfortunately, the analogue in our system could be ``forgot to
insert modified records into DHash.''
\item[Slave server can't load zone data.]
There are no slave servers in DDNS, so this cannot happen.
Further, replication is handled automatically, so there is no
equivalent in our system.
\item[Missing cache data.]
DDNS handles caching automatically via DHash, so this cannot happen.
\item[Loss of network connectivity.]
DDNS is robust against server failure or disconnection, so that loss of
an individual server cannot cripple name service for any part of the
hierarchy.
Unfortunately, DDNS is much worse off against network partitions.
For example, if a backhoe cuts MIT from the rest of the internet,
even though hosts on the internet will not see a disruption in any
part of the name space (not even MIT's names), hosts at MIT
may not even be able to look up their own names!
This could be addressed by having a local name ring as well
as the global internet name ring, but such a solution seems half-baked:
it brings back the hierarchy we eliminated.
\item[Missing subdomain delegation.]
In conventional DNS, if a domain's parent does not
update the domain's name server records in a timely fashion,
that domain will not be usable.  
DDNS partially eliminates this problem, since there are 
no need for name server records.
The analogue in DDNS is the domain's parent providing a signature
of that domain's public key RRSet.
At least in this case, there is no propagation delay: once the
parent sends you the signed public key, you can publish it yourself.
\item[Incorrect subdomain delegation.]
In conventional DNS, if a domain does not keep its parent informed
when name servers change names or IP addresses, eventually 
the number of correct name server entries held by the parent
will reach zero, at which point the domain is effectively dead.
DDNS partially eliminates this problem too, since it 
does not depend on name server records.
Even the analogue in DDNS is less likely, since the key record
cannot be changed without getting the changed signed by the parent.
An inconsistent RRSet cannot be inserted.
\end{description}

On a similar note, Jung {\it et al.}~\cite{dnscache:sigcommimw01} 
reported 23\% of 
DNS lookups failed to elicit any response, partially due to loops 
in name server resolution. 13\% of lookups result in a negative 
response, many of which are caused by NS records that point to 
non-existent or inappropriate hosts.
These can be quite subtle to detect; for example, an NS record that points
at a non-existent or wrong name server can cause lookups to fail
intermittently, depending on whether the server already has the 
record cached.
Since DDNS does not require name server records, this problem cannot happen.
Unfortunately, the analogue in DDNS would be improperly
configured public key records, which seem just as likely.
Any solution that applies to one problem
probably applies equally well to the other.

DDNS has better fault-tolerance over current DNS due to 
denial-of-service attacks. No longer is a few name servers need
to be taken down, before the records from that zone 
are inaccessible to others on the Internet. Because there is no 
name server heirarchy, the attacker has bring down a diverse
set of servers before data loss become apparent.

In summary, we believe that using a peer-to-peer system for
storing DNS records eliminates many common administrative
problems, but adds others.
The problems caused by network partition are particularly troubling.
It is not clear that DDNS is a net win over conventional DNS in
this arena.  At the least, conventional DNS is well understood.

\subsubsection{Dynamically-generated records}

Our system requires that all queries
can be anticipated in advance and their answers stored.
Since the {\tt hosts.txt} approach required this property
and the original DNS papers are silent on the topic,
it seems likely that this requirement was never 
explicitly intended to be relaxed.
However, the conventional DNS did relax the requirement:
since domains serve their own data, all possible queries need 
not be anticipated in advance as long as there is some algorithm
implemented in the server for responding to queries.
For example, to avoid the need to publish internal host names,
the name server for
{\em cs.bell-labs.com} will return a valid mail exchanger (MX) record
for any host name ending in {\em .cs.bell-labs.com}, even
for those that do not exist.

Additionally, responses can be tailored according to factors
other than the actual query.
For example, it is standard practice to randomly order the results
of a query to provide approximate load balancing~\cite{dns-load:rfc}.
As another example, content distribution networks like Akamai
use custom DNS responses both for real-time load balancing and
to route clients to nearby servers~\cite{akamai.rev.eng}.
% http://www.cs.washington.edu/homes/ratul/akamai.html

DDNS can provide none of these capabilities, which depend
on the coupling of the administrative hierarchy and the service structure.


