\section{Evaluation}

To evaluate the use of Chord to serve DNS data,
we used data from the study by Jung {\em et al.}\ on
a simulated network with 1000 DDNS nodes.
For the test, we turned off replication of records.
We did not simulate node failures,
so the only effect of replication would be to serve
as {\em a priori} caching.
Since the network is a fair amount smaller than the expected
size of a real Chord ring, replication would
bias the results in our favor because of the increased caching.

We first inserted answers to all the successful queries into the Chord network
and then ran a day's worth of successful queries
(approximately 260,000 queries),
measuring storage balance, load balance while answering
queries, and the number of RPCs required to perform each lookup.
The day's queries are heavy tailed.
There were about 120,000 distinct queries.  Of these, 93,100 appeared
just once.
Only 141 queries were executed more than a hundred times,
and only two were executed more than a thousand times.
The most common query, for {\em lcs.mit.edu}'s SOA record,
was executed 4219 times.

To simulate failed DNS queries, we started a similar network, did not insert
any data, and executed a day's worth of unresolved queries (approximately
220,000 queries), measuring
load balance and RPC counts.
These queries were distributed similarly to the successful queries.
Of 17,000 distinct queries, 8,000 were executed only once.
Only 165 queries were executed more than a hundred times,
and only eight were executed more than a thousand times.
The most common query, for {\em 39.246.224.209.in-addr.arpa}'s
PTR record, was executed 82,000 times.

Finally, to simulate popular entries, we ran what we called the ``slashdot test,''
inserting one record and then fetching it a hundred thousand times.

All three tests began each lookup at a random node in the Chord network.

\begin{figure}[h]
\epsfig{file=figures/ok.store.eps}
\caption{
Storage balance for 120,000 records later used as answers to
260,000 queries.
The graph shows a cumulative distribution for the number 
of records stored on each node.
Perfect balance would place the same number of records
on each node, making the cumulative distribution a vertical line
around 120.}
\label{fig:store}
\end{figure}

For the successful queries test, we inserted approximately 120,000
records to serve as answers to the 260,000 queries.
Figure~\ref{fig:store} shows that the median number of
records stored per node is about 120, as expected.
The distribution is exponential in both directions because
Chord nodes are randomly placed on a circle and store data
in proportion to the distance to their next neighbor clockwise
around the circle.  Irregularities in the random placement cause
some nodes to store more data than others.
The two nodes that stored in excess of 800
records (824 and 827) were both responsible for approximately
0.8\% of the circle, as compared with an expected 
responsibility of 0.1\%.
Even so, this irregularity drops off exponentially in both
directions, and can be partially addressed by having servers
run multiple nodes in proportion to their storage capacities.
We conclude that DDNS does an adequate job of balancing
storage among the peers.

\begin{figure}[h]
\epsfig{file=figures/both.fetch.eps}%,angle=270,width=3in}
\caption{Load balance for the three tests.
The graph shows a cumulative distribution for the number
of RPCs served per lookup by each node during the test.
Since there are a thousand nodes, ideal behavior would be
involving each node in $n/1000$ RPCs, where $n$
is the median number of RPCs executed per lookup (see Figure~\ref{fig:hops}).}
\label{fig:both-rpc}
\end{figure}

DHash's block caching helped provide load balance as
measured by RPCs served per lookup per node.
As shown in Figure~\ref{fig:both-rpc},
in the successful query test, nodes served RPCs in
approximate proportion to the number of records they stored.
Specifically, each node serves each of its popular blocks about
ten ($\log_2 1000$) times; after that, the block is cached
at enough other nodes that queries will find a cached copy
instead of reaching the responsible server.
A similar argument shows that very quickly every node has
a copy of incredibly popular blocks, as evidenced by the
Slashdot test: after the first few thousand requests, virtually
every node in the system has the record cached, so that
subsequent requests never leave the originating node.

For the unsuccessful query test, nodes served RPCs in 
proportion to the number of queries that expected the
desired record to reside on that node.
This does a worse job of load balancing since there is
no caching; for example, the node responsible for the
PTR record for {\em 39.246.224.209.in-addr.arpa} was
heavily loaded due to the 82,000 queries for that record.

%Athicha: I just added the following paragraph in 
%response to the normalized graph. The ones above are 
%unedited.
The graphs shows that the loads are similar for both
successful and unsuccessful queries, unlike in the current
DNS, where unresponsive queries might result in spurious 
retransmissions of requests.

\begin{figure}[h]
\epsfig{file=figures/hops.eps}%,angle=270,width=3in}
\caption{Client load to perform lookups. 
The graph shows a cumulative distribution for the number
of hops required for each lookup.}
\label{fig:hops}
\end{figure}

Figure~\ref{fig:hops} shows the number of RPCs required
by a client for various lookups.
Successful queries and unsuccessful queries have the same
approximately random distribution of hop counts, except
that successful queries usually end earlier due to finding
a cached copy of the block.
Since the slashdot record got cached everywhere very quickly,
virtually all lookups never left the requesting node.

\begin{figure}[h]
\epsfig{file=figures/03.ok.lat.eps}
\caption{Lookup latency distribution for successful one-server
queries over conventional DNS in the Jung.~{\em et al.}\ data.
Of the 260,000 successful lookups,
approximately 220,000 only queried one server.  The latencies for
those lookups are plotted.}
\label{fig:oklat}
\end{figure}

We would like to be able to compare the latency for DNS over Chord
with the latency for conventional DNS.  This is made difficult by the fact
that we do not have an Internet-wide Chord ring serving DNS records.
To compensate, we took the latency distribution measured in the Jung.~{\em et al.}\ study
and used it to compute the expected latencies of DNS over Chord
in a similar environment.

Figure~\ref{fig:oklat} shows the distribution of latency
for successful lookups in the Jung.~{\em et al.}\ one-day DNS trace.
Because we don't have individual latencies for requests that 
contacted multiple servers, only requests completed in one
round trip are plotted.
The tail of the graph goes out all the way to sixty seconds;
such slow servers would not be used in the Chord network,
since timeouts would identify them as having gone off the network.

Since such a small fraction of nodes have such long timeouts,
we cut the largest 2.5\% of latencies from the traces in order
to use them for our calculations.
To be fair, we also cut the smallest 2.5\% of the latencies 
from the traces, leaving the middle 95\%.

\begin{figure}[h]
\epsfig{file=figures/hopslat.eps}
\caption{Hops per lookup in the simulation of the Jung.~{\em et al.} traces
over Chord.}
\label{fig:hopslat}
\end{figure}

Figure~\ref{fig:hopslat} shows the correlation between
hops per lookup and expected latency.
For each hop count $h$, we randomly selected and summed $h$ latencies
from the Jung.~{\em et al.}\ distribution.
Each point in the graph is the average of 1000 random sums;
the bars show the standard deviations in each direction.

\begin{figure}[h]
\epsfig{file=figures/imaglat.ok.eps}
\caption{Imaginary latency to perform DNS lookups over Chord.
Assuming the latency distribution from the Jung.~{\em et al.} trace plotted
in Figure~\ref{fig:oklat}, this graph plots the latency distribution
of our simulated lookups over Chord.}
\label{fig:imaglat}
\end{figure}

Figure~\ref{fig:imaglat} displays the expected latency
in another way.
Here, for each query with hop count $h$, we chose a random
latency (the sum of $h$ random latencies from the Jung.~{\em et al.}\ distribution)
and used that as the query latency.
The cumulative distribution of these query latencies is plotted.

These experiments show that lookups in DDNS take longer
than lookups in conventional DNS: our median response time is 350ms
while conventional DNS's is about 43ms.
We have increased the median in favor of removing the
large tail: lookups taking 60s simply cannot happen
in our system.

%RSC: what does this mean?
% Do a measure of how long it takes to converge to good load 
% balancing when there is a burst of lookups for 1 (or more)
% popular names. Compare that to the slashdot effects now.

%\subsubsection{Latency}

%We cannot perform a fair measurement of latency so we will do an
%analysis based on counting number of hops between servers
%and combining that with real latency to current root DNS servers.

%XXAdding keys to resource records make them bigger
%XXPublic key cryptography is an expensive operation. 
%DDNS computes pub keys every time.

%DDNS is not going to disappear without an answer for a lookup,
%unlike in the current DNS. So we can't really compare this, but should
%mention about it.
