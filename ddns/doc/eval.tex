\section{Evaluation}

To evaluate the use of Chord to serve DNS data,
we used data from the study by Jung {\em et al.}\ on
a network with 1000 DDNS nodes.
We first inserted answers to all the successful queries into the Chord network
and then ran a day's worth of successful queries
(approximately 260,000 queries),
measuring storage balance, load balance while answering
queries, and the number of RPCs required to perform each lookup.

To simulate failed DNS queries, we started a similar network, did not insert
any data, and executed a day's worth of unresolved queries (approximately
220,000 queries), measuring
load balance and RPC counts.

Finally, to simulate popular entries, we ran what we call the ``slashdot test,''
inserting one record and then fetching it a hundred thousand times.

All three tests began each lookup at a random node in the Chord network.

\begin{figure}
\epsfig{file=figures/ok.store.eps,angle=270,width=3in}
\caption{
storage balance for 120,000 records later used as answers to
260,000 queries.
The graph shows a cumulative distribution for the number 
of records stored on each node.}
\label{fig:store}
\end{figure}

For the successful query test, we inserted approximately 120,000
records to serve as answers to the 260,000 queries.
Figure~\ref{fig:store} shows that the median number of
records stored per node is about 120, as expected.
The distribution is exponential in both directions because
Chord nodes are randomly placed on a circle and store data
in proportion to the distance to their next neighbor clockwise
around the circle.  Irregularities in the random placement cause
some nodes to store more data than others.
The two nodes that stored in excess of 800
records (824 and 827) were both responsible for approximately
0.8\% of the circle, as compared with an expected 
responsibility of 0.1\%.
Even so, this irregularity drops off exponentially in both
directions, and can be partially addressed by having servers
run multiple nodes in proportion to their storage capacities.
We conclude that DDNS does an adequate job of balancing
storage among the peers.

\begin{figure}
\epsfig{file=figures/both.fetch.eps,angle=270,width=3in}
\caption{load balance for the three tests.
The graph shows a cumulative distribution for the number
of RPCs served by each node during the test.}
\label{fig:both-rpc}
\end{figure}

DHash's block caching helped provide load balance as
measured by RPCs served per node.
As shown in figure~\ref{fig:both-rpc},
in the successful query test, nodes served RPCs in
approximate proportion to the number of records they stored.
Specifically, each node serves each of its popular blocks about
ten ($\log_2 1000$) times; after that, the block is cached
at enough other nodes that queries will find a cached copy
instead of reaching the responsible server.
A similar argument shows that very quickly every node has
a copy of incredibly popular blocks, as evidenced by the
Slashdot test: after the first few thousand requests, virtually
every node in the system has the record cached, so that
subsequent requests never leave the originating node.

For the unsuccessful query test, nodes served RPCs in 
proportion to the number of queries that expected the
desired record to reside on that node.
This does a worse job of load balancing since there is
no caching; for example, the node responsible for the
PTR record for 39.246.224.209.in-addr.arpa suffered
82,000 queries.

\begin{figure}
\epsfig{file=figures/hops.eps,angle=270,width=3in}
\caption{client work to perform lookups. 
The graph shows a cumulative distribution for the number
of hops required for each lookup.}
\label{fig:hops}
\end{figure}

Figure~\ref{fig:hops} shows the number of RPCs required
by a client for various lookups.
Successful queries and unsuccessful queries have the same
approximately random distribution of hop counts, except
that successful queries usually end earlier due to finding
a cached copy of the block.
Since the slashdot record got cached everywhere very quickly,
virtually all lookups never left the requesting node.


%RSC: what does this mean?
% Do a measure of how long it takes to converge to good load 
% balancing when there is a burst of lookups for 1 (or more)
% popular names. Compare that to the slashdot effects now.

%\subsubsection{Latency}

%We cannot perform a fair measurement of latency so we will do an
%analysis based on counting number of hops between servers
%and combining that with real latency to current root DNS servers.

%XXAdding keys to resource records make them bigger
%XXPublic key cryptography is an expensive operation. 
%DDNS computes pub keys every time.

%DDNS is not going to disappear without an answer for a lookup,
%unlike in the current DNS. So we can't really compare this, but should
%mention about it.
