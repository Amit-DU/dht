\section{Why (not) Cooperative DNS?}

Serving DNS data using peer-to-peer systems 
frees the domain owners from having to configure
or administer name servers. Anyone who 
wants to publish a domain only needs the higher-level
domain to sign her public key. DDNS takes care of
storing, serving, replicating, and caching of her DNS records.
Below, we discuss various issues in the current DNS, 
and the extent to which DDNS solves them.

\subsection{DNS Administration}

In their 1988 retrospective~\cite{dns}, Mockapetris and Dunlap
listed ``distribution of control vs. distribution of expertise
or responsibility'' as one of the shortcomings of their system,
lamenting:
\begin{quote}
Distributing authority for a database does not distribute
corresponding amounts of expertise.  Maintainers fix things
until they work, rather than until they work well, and want to use,
not understand, the systems they are provided.
Systems designers should anticipate this, and try to
compensate by technical means.
\end{quote}
To justify their point, they cited three failures:
they did not require proof that administrators had set up
truly redundant name servers without a single point of failure;
in the documentation, they used hour-long TTLs 
in the examples but suggested
day-long TTLs in the text, with the result that everyone used
hour-long TTLs; debugging was made difficult by not requiring
servers to identify their server version and type in an automated way.
DDNS eliminates much of the need for expertise by 
automatically providing a routing infrastructure for finding 
name information, automatically avoiding single points of 
failure.

In their handbook for the Berkeley DNS server, BIND,
Albitz and Liu~\cite{dns-bind} listed what they believed to be
thirteen of the most common problems in configuring
a name server.
Our system addresses six of them.

{\em Forget to signal primary master server.}
There are no primary servers in DDNS, so this cannot happen.
Unfortunately, the analogue in our system could be ``forget to
insert modified records into DDNS.''
Ultimately, we cannot protect against administrators
forgetting to commit their changes.

{\em Slave server cannot load zone data.}
DDNS solves this by automatically handling replication 
via the DHash protocol.
There are no slave servers.

{\em Missing cache data.}
Either by forgetting to install the cache file or accidentally deleting it,
system administrators can inadvertantly stop their name servers
from caching records.
DDNS automatically performs caching via DHash and does
not require explicit configuration of the cache.

{\em Loss of network connectivity.}
DDNS is robust against server failure or disconnection, so that loss of
an individual server cannot cripple name service for any part of the
hierarchy. Unfortunately, it suffers from 
network partitions. For example, if a backhoe cuts MIT from 
the rest of the internet,
even though hosts on the internet will not see a disruption in any
part of the name space (not even MIT's names), hosts at MIT
may not even be able to look up their own names! 
This problem is not new, since a client machine with no 
knowledge of MIT will require access to the root name servers
to get started.
Both systems partially avoid this problem with caching:
popular DNS data about local machines is likely to be cached
and thus available even after the partition.
DDNS actually works better in this situation, since the remaining
nodes will form a smaller Chord network and pool their caches.
% RSC: do we need to talk about healing the rings after a partition?
% i'd like to just sweep this under the rug; we're getting too far into
% Chord details.
%
% Pastry~\cite{pastry01} suggests the use of IP multicast to rediscover
% other peer-to-peer networks after IP connectivity is resumed,
% but IP multicast usually requires DNS XXX or something like that. XXX

{\em Missing subdomain delegation.}
In conventional DNS, a domain is not usable until its parent
has created the appropriate NS and glue records and
propagated them.
DDNS partially eliminates this problem, since there are 
no NS records.
In their place, the domain's parent would have to sign the domain's
public key RRSet.
At the least, this eliminates the propagation delay: once the
parent signs a domain's public key, it is up to the domain's administrator
to publish it.

{\em Incorrect subdomain delegation.}
In conventional DNS, if the parent is not notified 
when name servers or IP addresses of a domain change,
clients will eventually not be able to find the domain's name servers.
The analogue in DDNS would be a domain changing its public
key but forgetting to get its parent to sign the new key.
Without getting the signature, though, inserts of records signed
with the new key would fail.  This would alert the administrator
to the problem immediately.  (In conventional DNS, the problem
can go undetected since the local name server does not check
to see whether the parent domain correctly points at it.)

On a similar note, Jung {\it et al.}~\cite{dnscache:sigcommimw01} 
reported 23\% of 
DNS lookups failed to elicit any response, partially due to loops 
in name server resolution. 13\% of lookups result in a negative 
response, many of which are caused by NS records that point to 
non-existent or inappropriate hosts.

Conventional DNS requires that domain owners manage two types of
information: data about hosts ({\em e.g.}, A records)
and data about name service routing ({\em e.g.}, NS records).
The latter requires close coordination among servers in order
to maintain consistency; in practice this coordination often
does not happen, resulting in broken name service.
DDNS completely eliminates the need to maintain name
service routing data: routing information is automatically
maintained and updated by Chord without any human intervention.

In summary, we believe that using a peer-to-peer system for
storing DNS records eliminates many common administrative
problems, providing a much simpler way to serve DNS 
information.

\subsection{Dynamically generated records}

Our system requires that all queries
can be anticipated in advance and their answers stored.
Since the {\tt hosts.txt} approach required this property
and the original DNS papers are silent on the topic,
it seems likely that this requirement was never 
explicitly intended to be relaxed.
However, the conventional DNS did relax the requirement:
since domains serve their own data, all possible queries need 
not be anticipated in advance as long as there is some algorithm
implemented in the server for responding to queries.
For example, to avoid the need to publish internal host names,
the name server for
{\em cs.bell-labs.com} will return a valid mail exchanger (MX) record
for any host name ending in {\em .cs.bell-labs.com}, even
those that do not exist.

Additionally, responses can be tailored according to factors
other than the actual query.
For example, it is standard practice to randomly order the results
of a query to provide approximate load balancing~\cite{dns-load:rfc}.
As another example, content distribution networks like Akamai
use custom DNS responses both for real-time load balancing and
to route clients to nearby servers~\cite{akamai.rev.eng}.
% http://www.cs.washington.edu/homes/ratul/akamai.html

The system we have described
can provide none of these capabilities, which depend
on the coupling of the administrative hierarchy and the service
structure.
If some features were determined to be particularly desirable,
they could be implemented by the clients instead of the servers.

Another query type, ANY, requests all types of RRSets for
a given domain name. This does not cause performance 
degradation in the current DNS, since a name server
will have all the records for the domain names it serves.
In DDNS, we assemble all the records for the ANY 
request by looking up every record type for a domain 
name.
This results in a significant number of RPCs. 
We could optimize it by treating ANY just as any other type,
requiring that ANY records be stored just like A and MX records.
This invites inconsistency, though; since most ANY queries are
done by humans during debugging, we think the extra
delay is not a bad price to pay for the consistency guarantee.

\subsection{Denial of Service}

DDNS has better fault-tolerance due to denial-of-service attacks 
over the current DNS. 
Because there is no name server hierarchy, the 
attacker has to take down a diverse
set of servers before data loss becomes apparent.

Another type of denial of service is caused by 
a domain name owner inserting a large number 
of DNS records, using up space in the Chord network.
We can address this problem 
by enforcing a quota on how much data each organization
can insert depending on how much storage the organization
is contributing to DDNS. The rationale is that if you
have a lot of DNS entries, then you must contribute a 
corresponding amount of server resources.
% This policy is another incentive,
% in addition to the one above on network partition,
% for people to join the Chord ring.

